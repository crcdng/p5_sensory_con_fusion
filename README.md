# Sketch 23: Sensory Con-Fusion 
Experiments into mutual sensor disturbance

"Sensory Fusion" is a concept in AI/robotics where inputs from multiple physical sensors are combined in order to reduce uncertainty and optimize the performance of machine perception. A typical application is in self-driving cars with real-time data from different sources such as LIDAR (Light Detection and Ranging), radar and infrared. Others are Internet-of-Things scenarios or the coordination of multiple robots. Usually stochastic methods such as Kalman filters or random sets are deployed. 

In Sensory Con-Fusion, data streams from different modalities disturb and irritate each other, causing a synaesthetic nightmare. The project aims to challenge the predominant technological purpose of improvement and optimization. It invites debate about related topics like information overflow, autism and neural multisensory integration.

In this sketch, audio disturbs video: **in order to see, you and the environment have to be quiet**.

Written in Processing 3 [https://processing.org/](https://processing.org/).

This is work in progress (first version: May 3, 2017). The focus is on the conceptual not the aesthetic, and I have deliberately borrowed from the samples that come with Processing (see the individual sketches for details). I am also working on VR versions.

To run, copy the sketch folder into your Processing sketchbook. You might have to install libraries from the Processing GUI (`Sketch -> Import Library -> Add Library`).

Make sure to read the comments in the sketch files for details and current issues.
